<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Influence Functions Demo: Detailed Math & JavaScript</title>
  <!-- Configure MathJax to render LaTeX before the body loads -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: { fontCache: 'global' }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <!-- Basic CSS for side-by-side layout -->
  <style>
    body {
      margin: 0;
      font-family: sans-serif;
    }
    h1, h2, h3 {
      margin-top: 1em;
      margin-bottom: 0.5em;
    }
    .container {
      display: flex;
      flex-wrap: wrap;
      gap: 2rem;
      padding: 2rem;
    }
    .left-pane, .right-pane {
      flex: 1 1 45%;
      min-width: 300px;
    }
    .left-pane {
      border-right: 1px solid #ccc;
      padding-right: 1rem;
    }
    .right-pane {
      padding-left: 1rem;
    }
    pre, code {
      background: #f8f8f8;
      padding: 1rem;
      display: block;
      white-space: pre;
      border: 1px solid #ccc;
      border-radius: 4px;
    }
    button {
      margin-top: 1em;
      font-size: 1em;
      padding: 0.4em 0.8em;
      cursor: pointer;
    }
    .output {
      margin-top: 1em;
      padding: 1em;
      background: #f0f0ff;
      border: 1px solid #ccc;
      white-space: pre;
      min-height: 3em;
    }
    details {
      margin-top: 0.5em;
      margin-bottom: 1em;
      border: 1px solid #aaa;
      padding: 0.5em;
      border-radius: 4px;
      background-color: #fafafa;
    }
    details summary {
      font-weight: bold;
      cursor: pointer;
    }
    .panel-labels {
      font-size: 0.9em;
      background: #eee;
      border: 1px solid #ddd;
      padding: 0.4em;
      margin-bottom: 1em;
      border-radius: 4px;
    }
  </style>
</head>

<body>
  <h1 style="text-align:center;">
    Influence Functions Demo (Detailed Math on Left, Detailed JavaScript on Right)
  </h1>

  <div class="container">
    <!-- LEFT: Mathematical Derivation -->
    <div class="left-pane">
      <h2>Mathematical Derivation</h2>
      <div class="panel-labels">
        <strong>Panel Labels:</strong> [A] Predicted Probabilities, [B] Logistic Loss &amp; Gradient, [C] Hessian Matrix, [D] Influence Calculation
      </div>
      <p>
        We consider a <strong>logistic regression</strong> model with parameters 
        $w$ and $b$. The predicted probability for an input $x$ is given by
        $$\hat{y}(x)=\sigma(wx+b), \quad \text{with} \quad \sigma(z)=\frac{1}{1+e^{-z}}.$$
      </p>
      <p>
        Our training set is:
        $$ (x_0,y_0)=(0,0),\quad (x_1,y_1)=(1,0),\quad (x_2,y_2)=(2,1). $$
        We <em>fix</em> the parameters to
        $$w^*=2,\quad b^*=-3,$$
        so that we can perform explicit numerical calculations.
      </p>

      <!-- [A] Predicted Probabilities -->
      <h3>Step 1: Compute Predicted Probabilities <small>[A]</small></h3>
      <p>
        For each training point, we calculate 
        $$p_i=\sigma(w^*x_i+b^*).$$
        <br>
        For $x_0=0$: 
        $$z_0=2\cdot 0-3=-3,\quad p_0=\sigma(-3)=\frac{1}{1+e^{3}}\approx 0.0474.$$
        <br>
        For $x_1=1$: 
        $$z_1=2\cdot 1-3=-1,\quad p_1=\sigma(-1)=\frac{1}{1+e}\approx 0.2689.$$
        <br>
        For $x_2=2$: 
        $$z_2=2\cdot 2-3=1,\quad p_2=\sigma(1)=\frac{1}{1+e^{-1}}\approx 0.7311.$$
      </p>

      <!-- [B] Logistic Loss and Gradient -->
      <h3>Step 2: Logistic Loss and Gradient <small>[B]</small></h3>
      <p>
        The negative log-likelihood for an example $(x_i,y_i)$ is
        $$L_i(w,b)=-\Bigl[y_i\ln(\hat{y}_i)+(1-y_i)\ln(1-\hat{y}_i)\Bigr].$$
        Its gradient is given by
        $$\nabla_{(w,b)}L_i=(\hat{y}_i-y_i)
          \begin{pmatrix} x_i \\ 1 \end{pmatrix}.$$
      </p>
      <details>
        <summary>Derivation of the Gradient</summary>
        <p>
          The logistic loss for a single example is 
          $$L(w,b)=-\Bigl[y\ln(\sigma(z))+(1-y)\ln(1-\sigma(z))\Bigr],$$ 
          where 
          $$\sigma(z)=\frac{1}{1+e^{-z}}, \quad z=wx+b.$$
          Differentiating with respect to $z$ yields
          $$\frac{\partial L}{\partial z}=\sigma(z)-y.$$
          By the chain rule, since
          $$\frac{\partial z}{\partial w}=x \quad \text{and} \quad \frac{\partial z}{\partial b}=1,$$
          the gradient with respect to $(w,b)$ is
          $$\nabla_{(w,b)}L=(\sigma(z)-y)
             \begin{pmatrix} x \\ 1 \end{pmatrix}.$$
        </p>
      </details>

      <!-- [C] Hessian Matrix -->
      <h3>Step 3: Hessian Matrix <small>[C]</small></h3>
      <p>
        The Hessian for the training set is
        $$\nabla^2 L_{\text{train}}=\sum_{i}p_i(1-p_i)
           \begin{pmatrix} x_i^2 & x_i \\ x_i & 1 \end{pmatrix}.$$
        Compute the contributions:
        <br>
        For $i=0$ ($x_0=0$):
        $$p_0(1-p_0)\approx 0.0474 \cdot 0.9526 \approx 0.0452,$$
        yielding
        $$0.0452\begin{pmatrix}0 & 0\\ 0 & 1\end{pmatrix}=
           \begin{pmatrix}0 & 0\\ 0 & 0.0452\end{pmatrix}.$$
        <br>
        For $i=1$ ($x_1=1$):
        $$p_1(1-p_1)\approx 0.2689 \cdot 0.7311 \approx 0.1966,$$
        yielding
        $$0.1966\begin{pmatrix}1 & 1\\ 1 & 1\end{pmatrix}=
           \begin{pmatrix}0.1966 & 0.1966\\ 0.1966 & 0.1966\end{pmatrix}.$$
        <br>
        For $i=2$ ($x_2=2$):
        $$p_2(1-p_2)\approx 0.7311 \cdot 0.2689 \approx 0.1966,$$
        yielding
        $$0.1966\begin{pmatrix}4 & 2\\ 2 & 1\end{pmatrix}=
           \begin{pmatrix}0.7864 & 0.3932\\ 0.3932 & 0.1966\end{pmatrix}.$$
        <br>
        <em>Summing up the contributions:</em>
        $$\nabla^2 L_{\text{train}}\approx
           \begin{pmatrix}0+0.1966+0.7864 & 0+0.1966+0.3932\\[6pt]
                           0+0.1966+0.3932 & 0.0452+0.1966+0.1966\end{pmatrix}
           =\begin{pmatrix}0.9830 & 0.5898\\[4pt]0.5898 & 0.4384\end{pmatrix}.$$
      </p>
      <details>
        <summary>Derivation of the Hessian</summary>
        <p>
          Starting from the gradient for a single example,
          $$\nabla_{(w,b)}L=(\sigma(z)-y)
             \begin{pmatrix} x \\ 1 \end{pmatrix},$$
          we differentiate with respect to $(w,b)$.
          <br><br>
          First, note that 
          $$\frac{d}{dz}\sigma(z)=\sigma(z)(1-\sigma(z)).$$
          <br><br>
          Since $z=wx+b$, we have:
          $$\frac{\partial^2 L}{\partial w^2}=\sigma(z)(1-\sigma(z))\,x^2,$$
          $$\frac{\partial^2 L}{\partial w \partial b}=\sigma(z)(1-\sigma(z))\,x,$$
          $$\frac{\partial^2 L}{\partial b^2}=\sigma(z)(1-\sigma(z)).$$
          <br><br>
          Hence, for a single example, the Hessian is:
          $$\nabla^2_{(w,b)}L=\sigma(z)(1-\sigma(z))
             \begin{pmatrix} x^2 & x \\ x & 1 \end{pmatrix}.$$
          <br><br>
          For the entire training set, we sum over all examples:
          $$\nabla^2 L_{\text{train}}=\sum_i \sigma(z_i)(1-\sigma(z_i))
             \begin{pmatrix} x_i^2 & x_i \\ x_i & 1 \end{pmatrix}.$$
        </p>
      </details>

      <!-- [D] Influence Calculation -->
      <h3>Step 4: Influence Calculation <small>[D]</small></h3>
      <p>
        The influence of removing a training point $(x_i,y_i)$ on the test loss is
        $$\mathcal{I}_{\mathrm{up,loss}}(z_{\mathrm{test}},z_i)
           = -\,\nabla_\theta L(z_{\mathrm{test}})^\top\Bigl[\nabla^2L_{\mathrm{train}}\Bigr]^{-1}
           \nabla_\theta L(z_i).$$
        A negative value means that removing the point would increase the test loss (i.e., the point is beneficial),
        while a positive value indicates a harmful effect.
      </p>
    </div><!-- end left-pane -->

    <!-- RIGHT: JavaScript Computation -->
    <div class="right-pane">
      <h2>JavaScript Computation Code (Display Only)</h2>
      <div class="panel-labels">
        <strong>Panel Labels:</strong> [A] Test Point &amp; Training Gradients, [B] Hessian Calculation, [C] Hessian Inversion, [D] Influence Computation
      </div>
      <pre><code>
// Label [A]: Test Point and Training Gradients
// Fixed parameters: w = 2, b = -3
// Training set: (0,0), (1,0), (2,1)
// Test point: (1.5, 1)

function sigmoid(z) {
  return 1 / (1 + Math.exp(-z));
}

function gradSingleExample(w, b, x, y) {
  const z = w * x + b;
  const p = sigmoid(z);
  // Loss gradient: (p - y) * [x, 1]
  const grad = [(p - y) * x, (p - y)];
  return { z: z, p: p, grad: grad };
}

// Label [B]: Hessian Calculation
function hessian(w, b, X) {
  let H00 = 0, H01 = 0, H11 = 0;
  X.forEach(x => {
    const z = w * x + b;
    const p = sigmoid(z);
    const factor = p * (1 - p);
    H00 += factor * x * x;
    H01 += factor * x;
    H11 += factor;
  });
  return { H: [[H00, H01], [H01, H11]] };
}

// Label [C]: Hessian Inversion
function invert2x2(M) {
  const A = M[0][0], B = M[0][1], C = M[1][0], D = M[1][1];
  const det = A * D - B * C;
  if (Math.abs(det) < 1e-10) {
    throw new Error("Determinant too close to zero.");
  }
  return [
    [ D / det, -B / det ],
    [ -C / det, A / det ]
  ];
}

function mat2x2TimesVec(M, vec) {
  return [
    M[0][0] * vec[0] + M[0][1] * vec[1],
    M[1][0] * vec[0] + M[1][1] * vec[1]
  ];
}

function dot(u, v) {
  return u[0] * v[0] + u[1] * v[1];
}

// Label [D]: Influence Computation
function computeInfluences() {
  const X_train = [0, 1, 2];
  const y_train = [0, 0, 1];
  const X_test = 1.5, y_test = 1;
  const w = 2, b = -3;
  
  let output = "";
  output += "=== Influence Function Detailed Computation ===\n\n";
  
  // Label [A]
  output += "Label [A]: Fixed Parameters & Test Point Gradient\n";
  output += "  Parameters: w = " + w + ", b = " + b + "\n";
  output += "  For test point (x = " + X_test + ", y = " + y_test + "):\n";
  const testCalc = gradSingleExample(w, b, X_test, y_test);
  output += "    Compute z: z_test = " + w + " * " + X_test + " + (" + b + ") = " + testCalc.z.toFixed(6) + "\n";
  output += "    Compute p: p_test = sigmoid(z_test) = " + testCalc.p.toFixed(6) + "\n";
  output += "    Test gradient = (p_test - y_test) * [x, 1] = [ " + testCalc.grad[0].toFixed(6) + ", " + testCalc.grad[1].toFixed(6) + " ]\n\n";
  
  // Label [B]
  output += "Label [B]: Hessian Matrix Calculation\n";
  const hessResult = hessian(w, b, X_train);
  const H = hessResult.H;
  output += "  Hessian matrix:\n";
  output += "    [[ " + H[0][0].toFixed(6) + ", " + H[0][1].toFixed(6) + " ],\n";
  output += "     [ " + H[1][0].toFixed(6) + ", " + H[1][1].toFixed(6) + " ]]\n\n";
  
  // Label [C]
  output += "Label [C]: Inversion of the Hessian\n";
  const H_inv = invert2x2(H);
  output += "  Inverted Hessian:\n";
  output += "    [[ " + H_inv[0][0].toFixed(6) + ", " + H_inv[0][1].toFixed(6) + " ],\n";
  output += "     [ " + H_inv[1][0].toFixed(6) + ", " + H_inv[1][1].toFixed(6) + " ]]\n\n";
  
  // Label [D]
  output += "Label [D]: Influence Computation for Each Training Point\n";
  output += "  For each training example, influence = - (test gradient) dot (H_inv * training gradient)\n\n";
  
  X_train.forEach((x, i) => {
    const calc = gradSingleExample(w, b, x, y_train[i]);
    output += "  --- Training Point " + i + " (x = " + x + ", y = " + y_train[i] + ") ---\n";
    output += "    Compute z: z = " + w + " * " + x + " + (" + b + ") = " + calc.z.toFixed(6) + "\n";
    output += "    Compute p: p = sigmoid(z) = " + calc.p.toFixed(6) + "\n";
    output += "    Training gradient = (p - y) * [x, 1] = [ " + calc.grad[0].toFixed(6) + ", " + calc.grad[1].toFixed(6) + " ]\n";
    const Hinv_grad = mat2x2TimesVec(H_inv, calc.grad);
    output += "    H_inv * gradient = [ " + Hinv_grad[0].toFixed(6) + ", " + Hinv_grad[1].toFixed(6) + " ]\n";
    const infl = - dot(testCalc.grad, Hinv_grad);
    output += "    Influence = - (test gradient) dot (H_inv * training gradient) = " + infl.toFixed(6) + "\n\n";
  });
  
  output += "Note: A negative influence indicates that removing the training point would increase the test loss (i.e., the point is beneficial),\n";
  output += "      while a positive influence indicates that removing it would decrease the test loss (i.e., the point is harmful).\n";
  output += "=== End of Detailed Computation ===\n";
  return output;
}
</code></pre>
      <button id="btnCompute">Compute Detailed Influences</button>
      <div id="output" class="output"></div>
    </div><!-- end right-pane -->
  </div><!-- end container -->

  <!-- Executable JavaScript: Actual Function Definitions and Event Listener -->
  <script>
    // Define all functions so they are available globally

    function sigmoid(z) {
      return 1 / (1 + Math.exp(-z));
    }

    function gradSingleExample(w, b, x, y) {
      const z = w * x + b;
      const p = sigmoid(z);
      const grad = [(p - y) * x, (p - y)];
      return { z: z, p: p, grad: grad };
    }

    function hessian(w, b, X) {
      let H00 = 0, H01 = 0, H11 = 0;
      X.forEach(x => {
        const z = w * x + b;
        const p = sigmoid(z);
        const factor = p * (1 - p);
        H00 += factor * x * x;
        H01 += factor * x;
        H11 += factor;
      });
      return { H: [[H00, H01], [H01, H11]] };
    }

    function invert2x2(M) {
      const A = M[0][0], B = M[0][1], C = M[1][0], D = M[1][1];
      const det = A * D - B * C;
      if (Math.abs(det) < 1e-10) {
        throw new Error("Determinant too close to zero.");
      }
      return [
        [ D / det, -B / det ],
        [ -C / det, A / det ]
      ];
    }

    function mat2x2TimesVec(M, vec) {
      return [
        M[0][0] * vec[0] + M[0][1] * vec[1],
        M[1][0] * vec[0] + M[1][1] * vec[1]
      ];
    }

    function dot(u, v) {
      return u[0] * v[0] + u[1] * v[1];
    }

    function computeInfluences() {
      const X_train = [0, 1, 2];
      const y_train = [0, 0, 1];
      const X_test = 1.5, y_test = 1;
      const w = 2, b = -3;
      
      let output = "";
      output += "=== Influence Function Detailed Computation ===\n\n";
      
      // Label [A]
      output += "Label [A]: Fixed Parameters & Test Point Gradient\n";
      output += "  Parameters: w = " + w + ", b = " + b + "\n";
      output += "  For test point (x = " + X_test + ", y = " + y_test + "):\n";
      const testCalc = gradSingleExample(w, b, X_test, y_test);
      output += "    Compute z: z_test = " + w + " * " + X_test + " + (" + b + ") = " + testCalc.z.toFixed(6) + "\n";
      output += "    Compute p: p_test = sigmoid(z_test) = " + testCalc.p.toFixed(6) + "\n";
      output += "    Test gradient = (p_test - y_test) * [x, 1] = [ " + testCalc.grad[0].toFixed(6) + ", " + testCalc.grad[1].toFixed(6) + " ]\n\n";
      
      // Label [B]
      output += "Label [B]: Hessian Matrix Calculation\n";
      const hessResult = hessian(w, b, X_train);
      const H = hessResult.H;
      output += "  Hessian matrix:\n";
      output += "    [[ " + H[0][0].toFixed(6) + ", " + H[0][1].toFixed(6) + " ],\n";
      output += "     [ " + H[1][0].toFixed(6) + ", " + H[1][1].toFixed(6) + " ]]\n\n";
      
      // Label [C]
      output += "Label [C]: Inversion of the Hessian\n";
      const H_inv = invert2x2(H);
      output += "  Inverted Hessian:\n";
      output += "    [[ " + H_inv[0][0].toFixed(6) + ", " + H_inv[0][1].toFixed(6) + " ],\n";
      output += "     [ " + H_inv[1][0].toFixed(6) + ", " + H_inv[1][1].toFixed(6) + " ]]\n\n";
      
      // Label [D]
      output += "Label [D]: Influence Computation for Each Training Point\n";
      output += "  For each training example, influence = - (test gradient) dot (H_inv * training gradient)\n\n";
      
      X_train.forEach((x, i) => {
        const calc = gradSingleExample(w, b, x, y_train[i]);
        output += "  --- Training Point " + i + " (x = " + x + ", y = " + y_train[i] + ") ---\n";
        output += "    Compute z: z = " + w + " * " + x + " + (" + b + ") = " + calc.z.toFixed(6) + "\n";
        output += "    Compute p: p = sigmoid(z) = " + calc.p.toFixed(6) + "\n";
        output += "    Training gradient = (p - y) * [x, 1] = [ " + calc.grad[0].toFixed(6) + ", " + calc.grad[1].toFixed(6) + " ]\n";
        const Hinv_grad = mat2x2TimesVec(H_inv, calc.grad);
        output += "    H_inv * gradient = [ " + Hinv_grad[0].toFixed(6) + ", " + Hinv_grad[1].toFixed(6) + " ]\n";
        const infl = - dot(testCalc.grad, Hinv_grad);
        output += "    Influence = - (test gradient) dot (H_inv * training gradient) = " + infl.toFixed(6) + "\n\n";
      });
      
      output += "Note: A negative influence indicates that removing the training point would increase the test loss (i.e., the point is beneficial),\n";
      output += "      while a positive influence indicates that removing it would decrease the test loss (i.e., the point is harmful).\n";
      output += "=== End of Detailed Computation ===\n";
      return output;
    }

    document.getElementById("btnCompute").addEventListener("click", function() {
      try {
        const res = computeInfluences();
        document.getElementById("output").textContent = res;
      } catch (error) {
        document.getElementById("output").textContent = "Error: " + error.message;
      }
    });
  </script>
</body>
</html>
