{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence Functions Demo\n",
    "\n",
    "This notebook demonstrates the calculation of **influence functions** from the paper:\n",
    "**Koh and Liang** (ICML 2017) – \"Understanding Black-box Predictions via Influence Functions.\"\n",
    "\n",
    "We use a tiny logistic regression with just **2 parameters** (weight $w$ and bias $b$) on 3 training points.\n",
    "Then, we'll compute the *influence* of each training point on a particular test point.\n",
    "\n",
    "We'll do it **two ways**:\n",
    "1. **Direct Inversion** of the Hessian (only feasible here because our Hessian is just $2\\times2$!).\n",
    "2. **Conjugate Gradient (CG) Approach**, which generalizes to large models (no need to form or invert Hessian).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "We'll define a tiny dataset of 3 points:\n",
    "- $x=0, y=0$\n",
    "- $x=1, y=0$\n",
    "- $x=2, y=1$\n",
    "\n",
    "We'll also define a single test point $(x_{test} = 1.5,\\, y_{test} = 1)$.\n",
    "Then we implement basic logistic regression (negative log-likelihood) and the relevant derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# Tiny training set\n",
    "X_train = np.array([0.0, 1.0, 2.0])\n",
    "y_train = np.array([0,   0,   1  ])\n",
    "\n",
    "# One test point\n",
    "X_test = 1.5\n",
    "y_test = 1\n",
    "\n",
    "def loss_and_grad(w, b, X, y):\n",
    "    \"\"\"\n",
    "    Return (total_loss, grad_w, grad_b) for logistic regression on (X, y).\n",
    "    \"\"\"\n",
    "    m = len(X)\n",
    "    total_loss = 0.0\n",
    "    grad_w = 0.0\n",
    "    grad_b = 0.0\n",
    "    for i in range(m):\n",
    "        x_i = X[i]\n",
    "        y_i = y[i]\n",
    "        p_i = sigmoid(w * x_i + b)\n",
    "        # negative log-likelihood\n",
    "        total_loss += -(y_i * np.log(p_i + 1e-12) + (1-y_i)*np.log(1 - p_i + 1e-12))\n",
    "        # gradient\n",
    "        grad_w += (p_i - y_i)*x_i\n",
    "        grad_b += (p_i - y_i)\n",
    "    return total_loss, grad_w, grad_b\n",
    "\n",
    "def hessian(w, b, X, y):\n",
    "    \"\"\"\n",
    "    Return the 2x2 Hessian matrix of the logistic regression training loss.\n",
    "    \"\"\"\n",
    "    H = np.zeros((2, 2))\n",
    "    for i in range(len(X)):\n",
    "        x_i = X[i]\n",
    "        y_i = y[i]\n",
    "        p_i = sigmoid(w*x_i + b)\n",
    "        factor = p_i * (1 - p_i)\n",
    "        # single-example Hessian piece\n",
    "        # [[x_i^2, x_i], [x_i, 1]]\n",
    "        M = np.array([[x_i**2, x_i], [x_i, 1]])\n",
    "        H += factor * M\n",
    "    return H\n",
    "\n",
    "def grad_single_example(w, b, x, y):\n",
    "    \"\"\"\n",
    "    Gradient wrt (w, b) of the logistic loss for a single example (x, y).\n",
    "    Return np.array([dw, db]).\n",
    "    \"\"\"\n",
    "    p = sigmoid(w*x + b)\n",
    "    return np.array([(p - y)*x, (p - y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fitting the Model\n",
    "We'll do a simple gradient descent to find $(w^*, b^*)$. In practice you might use a library function or other optimizer. We'll just do a small loop until it (hopefully) converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted parameters: w*=6.8725, b*=-10.0661\n"
     ]
    }
   ],
   "source": [
    "def fit_logistic_regression(X, y, lr=0.1, max_iter=2000):\n",
    "    w, b = 0.0, 0.0\n",
    "    for _ in range(max_iter):\n",
    "        _, grad_w, grad_b = loss_and_grad(w, b, X, y)\n",
    "        w -= lr * grad_w\n",
    "        b -= lr * grad_b\n",
    "    return w, b\n",
    "\n",
    "# Fit on our tiny dataset\n",
    "w_star, b_star = fit_logistic_regression(X_train, y_train)\n",
    "print(f\"Fitted parameters: w*={w_star:.4f}, b*={b_star:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Direct Inversion of Hessian\n",
    "When the parameter dimension is small, we can just compute $H$ (2×2) and do `np.linalg.inv(H)`.\n",
    "\n",
    "The influence on test loss from removing training point $(x_i, y_i)$ is:\n",
    "$$\\displaystyle\\mathcal{I}_{\\text{up, loss}}(z_{test}, z_i) = -\\nabla_\\theta L(z_{test}, \\theta^*)^T\\, H_{\\theta^*}^{-1}\\,\\nabla_\\theta L(z_i, \\theta^*).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian at (w*, b*):\n",
      " [[0.13394749 0.08590028]\n",
      " [0.08590028 0.06191917]]\n",
      "Inverse Hessian:\n",
      " [[ 67.66686463 -93.87404164]\n",
      " [-93.87404164 146.38126799]]\n",
      "\n",
      "Influences (via direct Hessian inversion):\n",
      "  Training point 0 (x=0.0, y=0): 0.000104\n",
      "  Training point 1 (x=1.0, y=0): 0.228624\n",
      "  Training point 2 (x=2.0, y=1): -0.225480\n"
     ]
    }
   ],
   "source": [
    "# 1) Hessian at (w*, b*)\n",
    "H = hessian(w_star, b_star, X_train, y_train)\n",
    "H_inv = np.linalg.inv(H)\n",
    "print(\"Hessian at (w*, b*):\\n\", H)\n",
    "print(\"Inverse Hessian:\\n\", H_inv)\n",
    "\n",
    "# 2) Influence for each training point\n",
    "grad_test = grad_single_example(w_star, b_star, X_test, y_test)\n",
    "\n",
    "influences_direct = []\n",
    "for i in range(len(X_train)):\n",
    "    g_train_i = grad_single_example(w_star, b_star, X_train[i], y_train[i])\n",
    "    infl = - grad_test @ (H_inv @ g_train_i)\n",
    "    influences_direct.append(infl)\n",
    "\n",
    "print(\"\\nInfluences (via direct Hessian inversion):\")\n",
    "for i, infl in enumerate(influences_direct):\n",
    "    print(f\"  Training point {i} (x={X_train[i]}, y={y_train[i]}): {infl:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conjugate Gradient (CG) Approach\n",
    "For **larger** models, we cannot form or invert $H$. Instead, we only need to solve for $H^{-1} v$ for certain vectors $v$ (the gradient of a training point). Koh & Liang do this by **conjugate gradient**:\n",
    "\n",
    "1. We implement a function $\\mathrm{Hv}(v)$ that returns $H v$ (the Hessian-vector product).\n",
    "2. We use an iterative CG solver to find $x$ such that $Hx = v$ (which implies $x = H^{-1} v$).\n",
    "3. Then we do $-\\nabla_{\\theta} L(z_{test})^T x$ to get the influence.\n",
    "\n",
    "Below is a demo for our tiny 2D logistic regression. Of course, this is overkill here, but it shows how it generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influences (via conjugate gradient):\n",
      "  Training point 0 (x=0.0, y=0): -0.013410\n",
      "  Training point 1 (x=1.0, y=0): 0.005336\n",
      "  Training point 2 (x=2.0, y=1): -0.014046\n"
     ]
    }
   ],
   "source": [
    "def hessian_vector_product(w, b, X, y, v):\n",
    "    \"\"\"\n",
    "    Compute the Hessian-vector product:  (H v),\n",
    "    where H = sum_i p_i(1-p_i)*[[x_i^2, x_i],[x_i,1]].\n",
    "    v is a 2D vector [v_w, v_b].\n",
    "    \"\"\"\n",
    "    v_w, v_b = v\n",
    "    Hv = np.zeros(2)\n",
    "    for i in range(len(X)):\n",
    "        x_i = X[i]\n",
    "        y_i = y[i]\n",
    "        p_i = sigmoid(w*x_i + b)\n",
    "        factor = p_i * (1 - p_i)\n",
    "        # single-example Hessian block times v\n",
    "        # M = [[x_i^2, x_i],[x_i, 1]]\n",
    "        # M*v = [x_i^2*v_w + x_i*v_b, x_i*v_w + v_b]\n",
    "        Mv = np.array([x_i**2*v_w + x_i*v_b,\n",
    "                       x_i*v_w + v_b])\n",
    "        Hv += factor * Mv\n",
    "    return Hv\n",
    "\n",
    "def cg_solve(hessian_vector_prod_func, v, max_iter=50, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Solve Hx = v using conjugate gradient, where\n",
    "    hessian_vector_prod_func(x) = H x.\n",
    "    Returns x ~ H^-1 v.\n",
    "    \"\"\"\n",
    "    x = np.zeros_like(v)\n",
    "    r = v - hessian_vector_prod_func(x)\n",
    "    p = r.copy()\n",
    "    rsold = r @ r\n",
    "    for _ in range(max_iter):\n",
    "        Hp = hessian_vector_prod_func(p)\n",
    "        alpha = rsold / (p @ Hp + 1e-12)\n",
    "        x += alpha * p\n",
    "        r -= alpha * Hp\n",
    "        rsnew = r @ r\n",
    "        if np.sqrt(rsnew) < tol:\n",
    "            break\n",
    "        p = r + (rsnew/rsold)*p\n",
    "        rsold = rsnew\n",
    "    return x\n",
    "\n",
    "def influence_on_test_via_cg(w, b, X_train, y_train, x_test, y_test):\n",
    "    grad_test = grad_single_example(w, b, x_test, y_test)\n",
    "    influences = []\n",
    "    for i in range(len(X_train)):\n",
    "        g_train_i = grad_single_example(w, b, X_train[i], y_train[i])\n",
    "        # We'll define a small lambda that does H v\n",
    "        hvp_func = lambda vec: hessian_vector_product(w, b, X_train, y_train, vec)\n",
    "        # Solve Hx = grad_train_i => x = H^-1 grad_train_i\n",
    "        x_approx = cg_solve(hvp_func, g_train_i, max_iter=50, tol=1e-10)\n",
    "        # Influence = - grad_test^T x_approx\n",
    "        infl = - grad_test @ x_approx\n",
    "        influences.append(infl)\n",
    "    return influences\n",
    "\n",
    "# Compute via CG:\n",
    "influences_cg = influence_on_test_via_cg(w_star, b_star, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"Influences (via conjugate gradient):\")\n",
    "for i, infl in enumerate(influences_cg):\n",
    "    print(f\"  Training point {i} (x={X_train[i]}, y={y_train[i]}): {infl:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the CG-based results match the direct inversion results (within numerical precision)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- For **large models** (e.g., neural networks), you can't explicitly form or invert $H$. The CG approach is crucial because you only need the ability to compute $H v$ for any vector $v$ (which can be done via backprop twice in frameworks like PyTorch or TensorFlow).\n",
    "- In practice, you often approximate $H$ using a subset of data or use damping/regularization.\n",
    "- Negative influence typically indicates the training point *helped* the model for that test example (removing it would hurt performance), while positive influence means the training point was *hurting* performance for that test example.\n",
    "\n",
    "This concludes the demonstration!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
