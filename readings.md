## Week 2

Reading 1: Chancellor 2023.
- Citation: Chancellor, S., 2023. Toward practices for human-centered machine learning. Communications of the ACM, 66(3), pp.78-85.
- About: First, we'll read “Toward Practices for Human-Centered Machine Learning” by Stevie Chancellor, published in the Communications of the ACM. CACM is a venue in which experts in various fields of computing write broad pieces for the entire computing community.
- How to access: Visit https://cacm.acm.org/magazines/2023/3/270209-toward-practices-for-human-centered-machine-learning/fulltext

Reading 2: Mazmunder et al. 2022.
- Citation: Mazumder, M., Banbury, C., Yao, X., Karlaš, B., Gaviria Rojas, W., Diamos, S., Diamos, G., He, L., Parrish, A., Kirk, H.R. and Quaye, J., 2023. Dataperf: Benchmarks for data-centric ai development. Advances in Neural Information Processing Systems, 36, pp.5320-5347.
- About: Second, we'll read the Introduction of the DataPerfs paper, published in NeurIPS 2023 Datasets and Benchmarks Track.
- How to access: Visit https://arxiv.org/abs/2207.10062 

Response Instructions:

1) Please write one to two paragraphs describing why you’d like to work on, or with, ML/AI systems? You can imagine these paragraphs as text you might include in a cover letter.
2) Please list 1-3 “domains of interest” (e.g., social media, content recommendation, law, health care, mental health, the environment, economics). They can be at any level of granularity (e.g. “AI for health” is OK, as is “AI for oncology”). Similarly to part 1, the purpose of this is to help me identify trends in your interests so I can suggest optional readings that are of interest to you and your classmates!

If you submit any reasonable formatted submission for this reading response, you'll receive full credit. In future response instructions, you might see something along the lines of, "you must quote on of the readings directly to support your point").

For this reading response, you'll submit via CourSys.

Optional reading
- If interested in data-centric approach to large language models, check out this blog: https://sebastianraschka.com/blog/2023/optimizing-LLMs-dataset-perspective.html 

